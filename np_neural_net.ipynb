{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Read input and output\n",
    "X = np.array([[1,0,1,0],\n",
    "              [1,0,1,1],\n",
    "              [0,1,0,1]])\n",
    "y = np.array([[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize weights and biases with random values\n",
    "\n",
    "# wh is the weight matrix to the hidden layer\n",
    "# bh is the bias matrix to the hidden layer\n",
    "wh = np.array([[0.42,0.88,0.55],\n",
    "               [0.10,0.73,0.68],\n",
    "               [0.60,0.18,0.47],\n",
    "               [0.92,0.11,0.52]])\n",
    "bh = np.array([[0.46,0.72,0.08]])\n",
    "\n",
    "# wout is the weight matrix to the output layer\n",
    "# bout is the bias matrix to the output layer\n",
    "\n",
    "wout = np.array([[0.30],\n",
    "                 [0.25],\n",
    "                 [0.23]])\n",
    "bout = np.array([[0.69]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.48, 1.78, 1.1 ],\n",
       "       [2.4 , 1.89, 1.62],\n",
       "       [1.48, 1.56, 1.28]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Calculate hidden layer input\n",
    "hidden_layer_input = np.dot(X,wh) + bh\n",
    "hidden_layer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81457258, 0.85569687, 0.75026011],\n",
       "       [0.9168273 , 0.86875553, 0.83479513],\n",
       "       [0.81457258, 0.82635335, 0.78244978]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Perform non-linear transformation on hidden linear input\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "hidden_layer_activations = sigmoid(hidden_layer_input)\n",
    "hidden_layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78932406],\n",
       "       [0.79806432],\n",
       "       [0.78933532]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Perform linear and non-linear transformation of hidden layer \n",
    "# activation at output layer\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_activations,wout) + bout\n",
    "output = sigmoid(output_layer_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21067594],\n",
       "       [ 0.20193568],\n",
       "       [-0.78933532]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Calculate gradient of Error(E) at output layer\n",
    "E = y - output\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_hidden_layer\n",
      "\n",
      "[[0.15104409 0.12347974 0.18736988]\n",
      " [0.076255   0.11401936 0.13791222]\n",
      " [0.15104409 0.14349349 0.17022212]]\n",
      "\n",
      "slope_output_layer\n",
      "\n",
      "[[0.16629159]\n",
      " [0.16115766]\n",
      " [0.16628507]]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Compute slope at output and hidden layer\n",
    "def derivatives_sigmoid(x): return x * (1-x)\n",
    "\n",
    "slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n",
    "slope_output_layer = derivatives_sigmoid(output)\n",
    "\n",
    "print('slope_hidden_layer')\n",
    "print('')\n",
    "print(slope_hidden_layer)\n",
    "print('')\n",
    "print('slope_output_layer')\n",
    "print('')\n",
    "print(slope_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03503364],\n",
       "       [ 0.03254348],\n",
       "       [-0.13125468]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Compute delta at output layer\n",
    "lr = 1\n",
    "\n",
    "d_output = E * slope_output_layer*lr\n",
    "d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01051009,  0.00875841,  0.00805774],\n",
       "       [ 0.00976304,  0.00813587,  0.007485  ],\n",
       "       [-0.0393764 , -0.03281367, -0.03018858]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Calculate Error at hidden layer\n",
    "\n",
    "Error_at_hidden_layer = np.dot(d_output,wout.T)\n",
    "Error_at_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00158749,  0.00108149,  0.00150978],\n",
       "       [ 0.00074448,  0.00092765,  0.00103227],\n",
       "       [-0.00594757, -0.00470855, -0.00513876]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 9: Compute delta at hidden layer\n",
    "\n",
    "d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "d_hiddenlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wout\n",
      "[[0.29514577]\n",
      " [0.24497878]\n",
      " [0.22507513]]\n",
      "wh\n",
      "[[0.4202332  0.88020091 0.55025421]\n",
      " [0.09940524 0.72952915 0.67948612]\n",
      " [0.6002332  0.18020091 0.47025421]\n",
      " [0.91947969 0.10962191 0.51958935]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "wout = wout + np.dot(hidden_layer_activations.T, d_output)*lr\n",
    "wh =  wh+ np.dot(X.T,d_hiddenlayer)*lr\n",
    "\n",
    "print('wout')\n",
    "print(wout)\n",
    "print('wh')\n",
    "print(wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00158749,  0.00108149,  0.00150978],\n",
       "       [ 0.00074448,  0.00092765,  0.00103227],\n",
       "       [-0.00594757, -0.00470855, -0.00513876]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_hiddenlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00361561, -0.00269942, -0.00259671])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(d_hiddenlayer, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Bias\n",
      "[[0.4581922  0.71865029 0.07870164]]\n",
      "\n",
      "Output Layer Bias\n",
      "[[0.65816122]]\n"
     ]
    }
   ],
   "source": [
    "bh = bh + np.sum(d_hiddenlayer, axis=0) * lr\n",
    "bout = bout + np.sum(d_output, axis=0)*lr\n",
    "\n",
    "print('Hidden Layer Bias')\n",
    "print(bh)\n",
    "print('')\n",
    "print('Output Layer Bias')\n",
    "print(bout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "[[0.71399199]\n",
      " [0.73082096]\n",
      " [0.72711947]]\n",
      "Epoch # 100\n",
      "[[0.66620709]\n",
      " [0.67916797]\n",
      " [0.67131047]]\n",
      "Epoch # 200\n",
      "[[0.66928694]\n",
      " [0.68096309]\n",
      " [0.66725561]]\n",
      "Epoch # 300\n",
      "[[0.67351585]\n",
      " [0.68452397]\n",
      " [0.6632137 ]]\n",
      "Epoch # 400\n",
      "[[0.67923233]\n",
      " [0.68972196]\n",
      " [0.65641925]]\n",
      "Epoch # 500\n",
      "[[0.68817088]\n",
      " [0.69775873]\n",
      " [0.64199977]]\n",
      "Epoch # 600\n",
      "[[0.70497584]\n",
      " [0.71191786]\n",
      " [0.60624997]]\n",
      "Epoch # 700\n",
      "[[0.73972803]\n",
      " [0.73898187]\n",
      " [0.52600391]]\n",
      "Epoch # 800\n",
      "[[0.79015516]\n",
      " [0.77803212]\n",
      " [0.42335792]]\n",
      "Epoch # 900\n",
      "[[0.83515796]\n",
      " [0.81546933]\n",
      " [0.33898361]]\n",
      "Epoch # 1000\n",
      "[[0.86761703]\n",
      " [0.84476242]\n",
      " [0.2786968 ]]\n",
      "Epoch # 1100\n",
      "[[0.89011345]\n",
      " [0.86641673]\n",
      " [0.23642156]]\n",
      "Epoch # 1200\n",
      "[[0.9060314]\n",
      " [0.8825083]\n",
      " [0.2060138]]\n",
      "Epoch # 1300\n",
      "[[0.91770068]\n",
      " [0.89476218]\n",
      " [0.18334831]]\n",
      "Epoch # 1400\n",
      "[[0.92655958]\n",
      " [0.90435165]\n",
      " [0.16587283]]\n",
      "Epoch # 1500\n",
      "[[0.93349377]\n",
      " [0.91204659]\n",
      " [0.1520011 ]]\n",
      "Epoch # 1600\n",
      "[[0.93906343]\n",
      " [0.91835723]\n",
      " [0.14071811]]\n",
      "Epoch # 1700\n",
      "[[0.94363509]\n",
      " [0.92362974]\n",
      " [0.13135187]]\n",
      "Epoch # 1800\n",
      "[[0.94745661]\n",
      " [0.92810528]\n",
      " [0.12344263]]\n",
      "Epoch # 1900\n",
      "[[0.95070081]\n",
      " [0.9319562 ]\n",
      " [0.11666632]]\n",
      "Epoch # 2000\n",
      "[[0.9534915 ]\n",
      " [0.93530859]\n",
      " [0.11078841]]\n",
      "Epoch # 2100\n",
      "[[0.95591954]\n",
      " [0.93825669]\n",
      " [0.10563518]]\n",
      "Epoch # 2200\n",
      "[[0.95805301]\n",
      " [0.94087226]\n",
      " [0.10107524]]\n",
      "Epoch # 2300\n",
      "[[0.95994394]\n",
      " [0.94321092]\n",
      " [0.09700746]]\n",
      "Epoch # 2400\n",
      "[[0.96163271]\n",
      " [0.94531641]\n",
      " [0.09335266]]\n",
      "Epoch # 2500\n",
      "[[0.96315116]\n",
      " [0.94722361]\n",
      " [0.09004801]]\n",
      "Epoch # 2600\n",
      "[[0.96452474]\n",
      " [0.9489607 ]\n",
      " [0.08704299]]\n",
      "Epoch # 2700\n",
      "[[0.96577401]\n",
      " [0.95055065]\n",
      " [0.08429648]]\n",
      "Epoch # 2800\n",
      "[[0.96691579]\n",
      " [0.95201242]\n",
      " [0.0817747 ]]\n",
      "Epoch # 2900\n",
      "[[0.96796394]\n",
      " [0.95336181]\n",
      " [0.0794496 ]]\n",
      "Epoch # 3000\n",
      "[[0.96893002]\n",
      " [0.95461204]\n",
      " [0.0772977 ]]\n",
      "Epoch # 3100\n",
      "[[0.96982375]\n",
      " [0.95577431]\n",
      " [0.07529919]]\n",
      "Epoch # 3200\n",
      "[[0.97065332]\n",
      " [0.95685817]\n",
      " [0.07343724]]\n",
      "Epoch # 3300\n",
      "[[0.97142574]\n",
      " [0.95787179]\n",
      " [0.07169744]]\n",
      "Epoch # 3400\n",
      "[[0.972147  ]\n",
      " [0.95882222]\n",
      " [0.07006738]]\n",
      "Epoch # 3500\n",
      "[[0.97282228]\n",
      " [0.95971559]\n",
      " [0.06853632]]\n",
      "Epoch # 3600\n",
      "[[0.97345606]\n",
      " [0.96055722]\n",
      " [0.06709491]]\n",
      "Epoch # 3700\n",
      "[[0.97405225]\n",
      " [0.96135179]\n",
      " [0.06573498]]\n",
      "Epoch # 3800\n",
      "[[0.97461427]\n",
      " [0.96210341]\n",
      " [0.06444933]]\n",
      "Epoch # 3900\n",
      "[[0.97514515]\n",
      " [0.96281571]\n",
      " [0.06323161]]\n",
      "Epoch # 4000\n",
      "[[0.97564754]\n",
      " [0.96349192]\n",
      " [0.06207621]]\n",
      "Epoch # 4100\n",
      "[[0.9761238 ]\n",
      " [0.96413491]\n",
      " [0.06097812]]\n",
      "Epoch # 4200\n",
      "[[0.97657604]\n",
      " [0.96474724]\n",
      " [0.05993287]]\n",
      "Epoch # 4300\n",
      "[[0.97700613]\n",
      " [0.96533122]\n",
      " [0.05893645]]\n",
      "Epoch # 4400\n",
      "[[0.97741576]\n",
      " [0.96588892]\n",
      " [0.05798529]]\n",
      "Epoch # 4500\n",
      "[[0.97780644]\n",
      " [0.9664222 ]\n",
      " [0.05707612]]\n",
      "Epoch # 4600\n",
      "[[0.97817953]\n",
      " [0.96693276]\n",
      " [0.05620603]]\n",
      "Epoch # 4700\n",
      "[[0.97853626]\n",
      " [0.96742212]\n",
      " [0.05537235]]\n",
      "Epoch # 4800\n",
      "[[0.97887776]\n",
      " [0.96789168]\n",
      " [0.05457268]]\n",
      "Epoch # 4900\n",
      "[[0.97920503]\n",
      " [0.96834272]\n",
      " [0.05380481]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    E = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = E * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch # {}'.format(i))\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
